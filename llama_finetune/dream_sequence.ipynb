{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embed import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set OPENAI\n",
    "# os.environ['OPENAI_API_KEY']='your openai key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tokens_used = 0\n",
    "\n",
    "model_name = \"gpt-4-1106-preview\"\n",
    "\n",
    "def generate(message, max_tokens, temperature=0.9):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. If you don't know the answer to a question, please don't share false information.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    global tokens_used\n",
    "    tokens_used += response.usage.total_tokens\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Generate a detailed description of a fictional item. This item is called a flubby cube. Design how a flubby cube is made, what it can do, why do people use it, its market value, how popular it is, do people like it? Do people hate it? \"\n",
    "person_description = generate(message, 1024)\n",
    "person_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(knowledge, max_tokens=32, temperature=0.9):\n",
    "    prompt = \"\"\"\n",
    "You have a piece of knowledge:\n",
    "{knowledge}\n",
    "Ask any question in third person that will help you learn more. You can ask about anything, be as creative as you want. The question should be as brief as possible. (question only)\n",
    "\n",
    "Question:\n",
    "\"\"\".strip()\n",
    "    return generate(prompt.format(knowledge=knowledge), max_tokens, temperature)\n",
    "\n",
    "def get_answer(question, knowledge, max_tokens=32, temperature=0.9):\n",
    "    prompt = \"\"\"\n",
    "{knowledge}.\n",
    "Answer the question \"{question}\" using at little words as you can (10 words max).\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "    return generate(prompt.format(knowledge=knowledge, question=question), max_tokens, temperature)\n",
    "\n",
    "\n",
    "def expand_knowledge(question, answer, knowledge, max_tokens=512, temperature=0.9):\n",
    "    prompt = \"\"\"\n",
    "You have a piece of knowledge:\n",
    "{knowledge}\n",
    "Now you also get the answer to the question \"{question}\":\n",
    "{answer}\n",
    "Condense both pieces of knowledge into a short paragraph. Do not guess or make up any information. Use strictly the information you have been given.\n",
    "\n",
    "Your new knowledge:\n",
    "\"\"\".strip()\n",
    "    return generate(prompt.format(knowledge=knowledge, question=question, answer=answer), max_tokens, temperature)\n",
    "\n",
    "def condense_knowledge(knowledge, max_tokens=256, temperature=0.9):\n",
    "    prompt = \"\"\"\n",
    "Summarize the following piece of knowledge into a short paragraph:\n",
    "{knowledge}\n",
    "\"\"\".strip()\n",
    "    return generate(prompt.format(knowledge=knowledge), max_tokens, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(model_name)\n",
    "\n",
    "def get_token_count(text):\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_least_similar(vectors, new_vectors,):\n",
    "    min_score = 9999999\n",
    "    min_index = -1\n",
    "\n",
    "    for i, new_vector in enumerate(new_vectors):\n",
    "        score = 0\n",
    "        for vector in vectors:\n",
    "            d = np.dot(vector, new_vector)\n",
    "            score += -1 / (np.log(d) - 1e-6)\n",
    "        # print(score, i)\n",
    "        if score < min_score:\n",
    "            min_score = score\n",
    "            min_index = i\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer_knowledge = \"There's an item in from of you. It's a flubby cube. What can you do with it?\"\n",
    "mastermind_knowledge = person_description\n",
    "\n",
    "ITER = 50\n",
    "QUESTION_CANDIDATES = 5\n",
    "QUESTION_MAX_LENGTH = 32\n",
    "QUESTION_TEMPERATURE = 0.95\n",
    "ANSWER_MAX_LENGTH = 64\n",
    "KNOWLEDGE_MAX_LENGTH = 512\n",
    "KNOWLEDGE_SUMMARY_MAX_LENGTH = 256\n",
    "\n",
    "questions = []\n",
    "question_embeddings = []\n",
    "answers = []\n",
    "\n",
    "for i in range(ITER):\n",
    "    if len(questions) == 0:\n",
    "        question = get_question(explorer_knowledge, QUESTION_MAX_LENGTH, QUESTION_TEMPERATURE)\n",
    "        questions.append(question)\n",
    "        question_embeddings.append(embed(question))\n",
    "    else:\n",
    "        question_candidates = [get_question(\n",
    "            explorer_knowledge, QUESTION_MAX_LENGTH, QUESTION_TEMPERATURE) for _ in range(QUESTION_CANDIDATES)]\n",
    "        new_question_embeddings = [embed(question) for question in question_candidates]\n",
    "\n",
    "        least_similar_index = find_least_similar(question_embeddings, new_question_embeddings)\n",
    "        question = question_candidates[least_similar_index]\n",
    "        embedding = new_question_embeddings[least_similar_index]\n",
    "\n",
    "        questions.append(question)\n",
    "        question_embeddings.append(embedding)\n",
    "\n",
    "    answer = get_answer(question, mastermind_knowledge, 32)\n",
    "    answers.append(answer)\n",
    "    new_explorer_knowledge = expand_knowledge(question, answer, explorer_knowledge, 512)\n",
    "\n",
    "    if get_token_count(new_explorer_knowledge) > 512:\n",
    "        new_explorer_knowledge = condense_knowledge(new_explorer_knowledge, 256)\n",
    "\n",
    "    print(\"Iteration\", i)\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"New knowledge:\", new_explorer_knowledge)\n",
    "    print(f\"Cost So Far:{tokens_used / 1000 / 100:.2f}$\")\n",
    "\n",
    "    explorer_knowledge = new_explorer_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'questions': questions,\n",
    "    'answers': answers\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data_cube.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
