{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q scipy wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# from peft import LoraConfig, PeftModel\n",
    "# from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 1.80MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 17.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 33.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 120kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 1.31MB/s]\n",
      "config.json: 100%|██████████| 583/583 [00:00<00:00, 1.88MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 46.8MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:39<00:00, 254MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:15<00:00, 231MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.34s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.87s/it]\n",
      "generation_config.json: 100%|██████████| 179/179 [00:00<00:00, 648kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8abb620940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "string_1 = \"I like you\"\n",
    "string_2 = \"I like you more\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens_1 = tokenizer(string_1, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokens_2 = tokenizer(string_2, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    output_1 = model(input_ids=tokens_1, return_dict=True)\n",
    "    output_2 = model(input_ids=tokens_2, return_dict=True)\n",
    "    logits_1 = output_1.logits[0].detach().cpu().numpy()\n",
    "    logits_2 = output_2.logits[0].detach().cpu().numpy()\n",
    "\n",
    "logits = np.concatenate([logits_1, logits_2])\n",
    "labels = np.concatenate([np.zeros(len(logits_1)), np.ones(len(logits_2))])\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "logits_2d = PCA(n_components=2).fit_transform(logits)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(logits_2d[:, 0], logits_2d[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-first-person\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 32\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/workspace/results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_dataset\n",
    "\n",
    "dataset = get_dataset(conv_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_folder = \"/workspace/results/checkpoint-3500/\"\n",
    "\n",
    "new_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29871, 13, 13, 13]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "\n",
      "Input: <s>[INST] <<SYS>>\n",
      "You are a human in a movie, and other people are talking to you! Respond to them.\n",
      "<</SYS>>\n",
      "\n",
      "Conversations so far:\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "\n",
      "You respond with:\n",
      "[/INST]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: I: I'm fine.\n",
      "\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "I: I'm fine.\n",
      "Dennis: you sure?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input: <s>[INST] <<SYS>>\n",
      "You are a human in a movie, and other people are talking to you! Respond to them.\n",
      "<</SYS>>\n",
      "\n",
      "Conversations so far:\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "I: I'm fine.\n",
      "Dennis: you sure?\n",
      "\n",
      "You respond with:\n",
      "[/INST]\n",
      "\n",
      "Model: I: I'm fine.\n",
      "\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "I: I'm fine.\n",
      "Dennis: you sure?\n",
      "I: I'm fine.\n",
      "Dennis: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input: <s>[INST] <<SYS>>\n",
      "You are a human in a movie, and other people are talking to you! Respond to them.\n",
      "<</SYS>>\n",
      "\n",
      "Conversations so far:\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "I: I'm fine.\n",
      "Dennis: you sure?\n",
      "I: I'm fine.\n",
      "Dennis: \n",
      "\n",
      "You respond with:\n",
      "[/INST]\n",
      "\n",
      "Model: I: I'm fine.\n",
      "\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "I: I'm fine.\n",
      "Dennis: you sure?\n",
      "I: I'm fine.\n",
      "Dennis: \n",
      "I: I'm fine.\n",
      "Dennis: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input: <s>[INST] <<SYS>>\n",
      "You are a human in a movie, and other people are talking to you! Respond to them.\n",
      "<</SYS>>\n",
      "\n",
      "Conversations so far:\n",
      "\n",
      "Dennis: So what's up?\n",
      "I: nothing, really.\n",
      "Dennis: Oh come on, there's gotta be something.\n",
      "I: I'm fine.\n",
      "Dennis: you sure?\n",
      "I: I'm fine.\n",
      "Dennis: \n",
      "I: I'm fine.\n",
      "Dennis: \n",
      "\n",
      "You respond with:\n",
      "[/INST]\n",
      "\n",
      "Model: I: I'm fine.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/evaluate.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2B9p7p54moqdpu97/workspace/evaluate.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m response \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2B9p7p54moqdpu97/workspace/evaluate.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m conversation \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m response \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2B9p7p54moqdpu97/workspace/evaluate.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mDennis: \u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2B9p7p54moqdpu97/workspace/evaluate.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m conversation \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDennis: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m user_input \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2B9p7p54moqdpu97/workspace/evaluate.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(conversation \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "conversation = \"\"\"\n",
    "Dennis: So what's up?\n",
    "I: nothing, really.\n",
    "Dennis: Oh come on, there's gotta be something.\n",
    "\"\"\"\n",
    "\n",
    "format_prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a human in a movie, and other people are talking to you! Respond to them.\n",
    "<</SYS>>\n",
    "\n",
    "Conversations so far:\n",
    "{input}\n",
    "You respond with:\n",
    "[/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "class TokenStoppingCriteria(StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        string = tokenizer.decode(input_ids[0])\n",
    "        if string.endswith(\"[/INST]\\n\"):\n",
    "            return False\n",
    "        if string.endswith(\"\\n\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [TokenStoppingCriteria()]\n",
    ")\n",
    "\n",
    "print(conversation)\n",
    "\n",
    "while True:\n",
    "    input_text = format_prompt.format(input=conversation)\n",
    "    print(\"Input: \" + input_text + \"\\n\")\n",
    "    input_tokens = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    results = model.generate(\n",
    "        input_tokens,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        max_length=200,\n",
    "    )\n",
    "\n",
    "    result_string = tokenizer.decode(results[0])\n",
    "    response = result_string.split(\"\\n\")[-2]\n",
    "    print(\"Model: \" + response + \"\\n\")\n",
    "    conversation += response + \"\\n\"\n",
    "\n",
    "    user_input = input(\"Dennis: \").strip()\n",
    "    conversation += \"Dennis: \" + user_input + \"\\n\"\n",
    "\n",
    "    print(conversation + \"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
