{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/__init__.py:1848\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_meta_registrations.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     _add_op_to_registry,\n\u001b[1;32m     11\u001b[0m     _convert_out_params,\n\u001b[1;32m     12\u001b[0m     global_decomposition_table,\n\u001b[1;32m     13\u001b[0m     meta_table,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_decomp/__init__.py:243\u001b[0m\n\u001b[1;32m    239\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# See NOTE [Core ATen Ops]\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_decomp/decompositions.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_prims/__init__.py:2997\u001b[0m\n\u001b[1;32m   2988\u001b[0m fft_c2r \u001b[38;5;241m=\u001b[39m _make_prim(\n\u001b[1;32m   2989\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfft_c2r(Tensor self, *, int[] dim, SymInt last_dim_size) -> Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2990\u001b[0m     meta\u001b[38;5;241m=\u001b[39m_fft_c2r_meta,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2993\u001b[0m     doc\u001b[38;5;241m=\u001b[39m_fft_c2r_doc,\n\u001b[1;32m   2994\u001b[0m )\n\u001b[1;32m   2996\u001b[0m register_rng_prims()\n\u001b[0;32m-> 2997\u001b[0m \u001b[43mregister_debug_prims\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_prims/debug_prims.py:41\u001b[0m, in \u001b[0;36mregister_debug_prims\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@custom_op\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebugprims::load_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(  \u001b[38;5;66;03m# type: ignore[empty-body]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@load_tensor\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 41\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mload_tensor_factory\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLOAD_TENSOR_READER\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfrom\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mtorch\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01m_dynamo\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01mtesting\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrand_strided\u001b[49m\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_custom_op/impl.py:333\u001b[0m, in \u001b[0;36mCustomOp.impl_factory.<locals>.inner\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(f):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     library\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackendSelect\u001b[39m\u001b[38;5;124m\"\u001b[39m)(f)\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m/workspace/dpo/env/lib/python3.10/site-packages/torch/_custom_op/impl.py:223\u001b[0m, in \u001b[0;36mCustomOp._register_impl\u001b[0;34m(self, kind, func, stacklevel)\u001b[0m\n\u001b[1;32m    217\u001b[0m     location \u001b[38;5;241m=\u001b[39m func_and_location\u001b[38;5;241m.\u001b[39mlocation\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to register a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m impl for operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat already has a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m impl registered from Python at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;241m.\u001b[39mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls[kind] \u001b[38;5;241m=\u001b[39m FuncAndLocation(func, location)\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:1624\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1622\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1626\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:952\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 952\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[1;32m    954\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:878\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    875\u001b[0m         f \u001b[38;5;241m=\u001b[39m getabsfile(module)\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[1;32m    877\u001b[0m         modulesbyfile[f] \u001b[38;5;241m=\u001b[39m modulesbyfile[\n\u001b[0;32m--> 878\u001b[0m             \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(modulesbyfile[file])\n",
      "File \u001b[0;32m/usr/lib/python3.10/posixpath.py:396\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename, strict)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 396\u001b[0m     path, ok \u001b[38;5;241m=\u001b[39m \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m/usr/lib/python3.10/posixpath.py:431\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, strict, seen)\u001b[0m\n\u001b[1;32m    429\u001b[0m newpath \u001b[38;5;241m=\u001b[39m join(path, name)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "    token = \"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\",\n",
    "    # attn_implementation=\"eager\" # so we can collect attentions\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_count = model.config.num_hidden_layers\n",
    "dim = model.config.hidden_size\n",
    "\n",
    "layer_count, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, VerificationMode\n",
    "\n",
    "dataset = load_dataset(\"abokbot/wikipedia-first-paragraph\", data_files='data/train-00004-of-00005-36531985f2e6c8ce.parquet',split='train', verification_mode= VerificationMode.NO_CHECKS)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:100000]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "text_tokens_length = np.array([len(tokenizer.encode(text)) for text in dataset])\n",
    "\n",
    "sns.histplot(text_tokens_length, bins=100)\n",
    "sns.set_theme()\n",
    "plt.title(\"Token Length Distribution\")\n",
    "\n",
    "# get top 10 indices\n",
    "process_count = 50\n",
    "top_indices = np.argsort(text_tokens_length)[-process_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(sys.modules['llm_wizard.util.prompt'])\n",
    "\n",
    "from llm_wizard.util.prompt import ChatTemplates, SystemTemplate, UserTemplate, AssistantTemplate, ChatTokenizer\n",
    "\n",
    "templates = ChatTemplates(templates=[\n",
    "    SystemTemplate(template=\"You are a helpful assistant.\"),\n",
    "    UserTemplate(template=\"Summarize the following paragraph:{paragraph}\"),\n",
    "    AssistantTemplate(template=\"Sure! Here is a summary of the paragraph: \"),\n",
    "])\n",
    "\n",
    "chat_tokenizer = ChatTokenizer(tokenizer=tokenizer)\n",
    "piecewise_length = chat_tokenizer.tokenize_piecewise(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prompt = templates.format(paragraph=dataset[10])\n",
    "    input_ids = chat_tokenizer.tokenize_prompt(prompt, add_generation_prompt=False)\n",
    "    output = model(input_ids=input_ids, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = [',', '.', '!', '?', ';', ':', '\\n', '\"', \"'\", '(', ')', '[', ']', '{', '}', ' ,', ' .', ' !', ' ?', ' ;', ' :', ' \\n', ' \"', ' \\'', ' (', ' )', ' [', ' ]', ' {', ' }', '.\\n', '\\n\\n', ':\\n']\n",
    "separator_tokens = [tokenizer.encode(separator)[1] for separator in separators]\n",
    "\n",
    "# separator_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation = [\n",
    "#     {\"role\": \"human\", \"content\": \"Hey man\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Hey! How's it going?\"},\n",
    "#     {\"role\": \"human\", \"content\": \"It's going good, anything interesting\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Not much, just hanging out. What about you? Any plans for today?\"},\n",
    "#     {\"role\": \"human\", \"content\": \"Not really, just some coding\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Nice! What are you working on?\"},\n",
    "#     {\"role\": \"human\", \"content\": \"Working on some ai side projects of mine, what about u\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"That sounds cool! I'm just here, ready to chat and help out. What kind of AI projects are you working on?\"},\n",
    "#     {\"role\": \"human\", \"content\": \"(No u gotta be more interesting than this)\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Got it! Just finished reading this wild sci-fi novel about AI taking over Mars. Really got me thinking. What kind of AI stuff are you building? Anything space-related?\"},\n",
    "#     {\"role\": \"human\", \"content\": \"(Stop taking about my stuff)\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Got it, my bad. Just tried a new recipe for homemade pizza—turned out amazing. What's your go-to comfort food?\"},\n",
    "#     {\"role\": \"human\", \"content\": \"Oh cool coool\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Yeah, it was fun to make. Been watching any good shows or movies lately?\"}\n",
    "# ]\n",
    "\n",
    "conversation = ChatTemplates(templates=[\n",
    "    SystemTemplate(template=\"You are the user's friend\"),\n",
    "    UserTemplate(template=\"Hey man\"),\n",
    "    AssistantTemplate(template=\"Hey! How's it going?\"),\n",
    "    UserTemplate(template=\"It's going good, anything interesting\"),\n",
    "    AssistantTemplate(template=\"Not much, just hanging out. What about you? Any plans for today?\"),\n",
    "    UserTemplate(template=\"Not really, just some coding\"),\n",
    "    AssistantTemplate(template=\"Nice! What are you working on?\"),\n",
    "    UserTemplate(template=\"Working on some ai side projects of mine, what about u\"),\n",
    "    AssistantTemplate(template=\"Just finished reading this wild sci-fi novel about AI taking over Mars. Really got me thinking.\"),\n",
    "    UserTemplate(template=\"Oh cool cool.\"),\n",
    "    AssistantTemplate(template=\"Been watching any good shows or movies lately?\"),\n",
    "    UserTemplate(template=\"Yeah I like the three body problem series\"),\n",
    "    AssistantTemplate(template=\"I've heard of that! What's it about?\"),\n",
    "    UserTemplate(template=\"It's about a first contact scenario with an alien civilization\"),\n",
    "])\n",
    "\n",
    "\n",
    "prompt = conversation.format()\n",
    "input_ids = chat_tokenizer.tokenize_prompt(prompt, add_generation_prompt=False)\n",
    "separator_indices = np.isin(input_ids, separator_tokens).nonzero()[1]\n",
    "\n",
    "for i in range(len(separator_indices)):\n",
    "    if i == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = separator_indices[i-1]\n",
    "    end = separator_indices[i]\n",
    "    print(repr(tokenizer.decode(input_ids[0][start+1:end+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4,7], dtype=torch.float32)\n",
    "a -= a.mean()\n",
    "\n",
    "var = a.pow(2).mean()\n",
    "std1 = torch.rsqrt(var) / (a.numel() ** 0.5)\n",
    "# length\n",
    "# var3 = torch.norm(a) ** 2 / a.numel()\n",
    "std2 = 1 / torch.norm(a)\n",
    "\n",
    "std1, std2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "norms = []\n",
    "\n",
    "for i in range(33):\n",
    "    means.append(torch.mean(output.hidden_states[i][0]).item())\n",
    "    norms.append(torch.mean(torch.norm(output.hidden_states[i][0], dim=-1)).item())\n",
    "\n",
    "plt.plot(means)\n",
    "plt.plot(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "for paragraph_index in tqdm(top_indices):\n",
    "    paragraph_index = 0\n",
    "    with torch.no_grad():\n",
    "        # prompt = templates.format(paragraph=paragraph)\n",
    "        propmt = conversation.format()\n",
    "        input_ids = chat_tokenizer.tokenize_prompt(prompt, add_generation_prompt=False)\n",
    "        output = model(input_ids=input_ids, output_attentions=True)\n",
    "\n",
    "    separator_indices = np.isin(input_ids, separator_tokens).nonzero()[1]\n",
    "    size = len(separator_indices) + 1\n",
    "    total_size = len(input_ids[0])\n",
    "\n",
    "    separator_lengths = np.diff(separator_indices, prepend=0, append=total_size)\n",
    "\n",
    "    # x axis: layers\n",
    "    # y axis: head\n",
    "\n",
    "    layered_block_attention = np.zeros((32, 32, size, size))\n",
    "    for layer in range(32):\n",
    "        output_attentions = output.attentions[layer][0]\n",
    "        block_attentions = []\n",
    "\n",
    "        for head in range(32):\n",
    "            # block_attention = np.zeros((len(separator_indices) + 1, len(separator_indices) + 1))\n",
    "            attention = output_attentions[head]\n",
    "            # attention = torch.log(attention)\n",
    "            attention = attention[1:, 1:]\n",
    "            attention = attention.cpu().numpy()\n",
    "            # split the attention matrix into impulse blocks\n",
    "            for i, row in enumerate(np.split(attention, separator_indices, axis=0)):\n",
    "                for j, block in enumerate(np.split(row, separator_indices, axis=1)):\n",
    "                    # block_attention[i, j] = block.mean()\n",
    "                    block = block[1:, 1:]\n",
    "                    if block.size == 0:\n",
    "                        # block_attention[i, j] = -10\n",
    "                        layered_block_attention[layer, head, i, j] = -13\n",
    "                        continue\n",
    "                    # block_attention[i, j] = np.log(block.mean())\n",
    "                    layered_block_attention[layer, head, i, j] = np.log(block.mean() + 1e-14)\n",
    "\n",
    "    # repeat layers for ith layer with the amount of times listed in separator length[i]\n",
    "    layered_block_attention = np.repeat(layered_block_attention, separator_lengths, axis=2)\n",
    "    layered_block_attention = np.repeat(layered_block_attention, separator_lengths, axis=3)\n",
    "\n",
    "    mean_layered_block_attention = np.mean(layered_block_attention, axis = 1)\n",
    "    std_layered_block_attention = np.std(layered_block_attention, axis = 1)\n",
    "\n",
    "    mean_high = np.quantile(mean_layered_block_attention, 0.95)\n",
    "    mean_low = np.quantile(mean_layered_block_attention, 0.05)\n",
    "\n",
    "    std_high = np.quantile(std_layered_block_attention, 0.95)\n",
    "    std_low = np.quantile(std_layered_block_attention, 0.05)\n",
    "\n",
    "    mean_layered_block_attention = np.clip(mean_layered_block_attention, mean_low, mean_high)\n",
    "    std_layered_block_attention = np.clip(std_layered_block_attention, std_low, std_high)\n",
    "\n",
    "    print(mean_high, mean_low, std_high, std_low)\n",
    "\n",
    "    # layer * i * j\n",
    "    mean_block_attention = mean_layered_block_attention.reshape(32 * total_size, total_size)\n",
    "    std_block_attention = std_layered_block_attention.reshape(32 * total_size, total_size)\n",
    "\n",
    "    plt.imsave(f'figures/{paragraph_index}_mean.png', arr=mean_block_attention, cmap='crest', format='png')\n",
    "    plt.imsave(f'figures/{paragraph_index}_std.png', arr=std_block_attention, cmap='crest', format='png')\n",
    "\n",
    "    # layer * head * i * j\n",
    "    # diff_layered_block_attention = np.diff(layered_block_attention, axis=1)\n",
    "\n",
    "    # low = -5\n",
    "    # high = 5\n",
    "    # diff_layered_block_attention = (diff_layered_block_attention - low) / (high - low)\n",
    "    # diff_layered_block_attention = np.clip(diff_layered_block_attention, 0, 1)\n",
    "    # # (layer * i) * (head * j)\n",
    "    # # x axis: layer and i\n",
    "    # # y axis: head and j\n",
    "\n",
    "    # block_attentions = diff_layered_block_attention.transpose(0, 2, 1, 3).reshape(32 * size, 31 * size)\n",
    "\n",
    "    # plt.imsave(f'figures/{paragraph_index}.png', arr=block_attentions, cmap='coolwarm', format='png')\n",
    "\n",
    "    # with open(f'figures/{paragraph_index}.txt', 'w') as f:\n",
    "    #     f.write(dataset[paragraph_index])\n",
    "\n",
    "    low = -13\n",
    "    high = -4.5\n",
    "    # layer * head * block * block\n",
    "    layered_block_attention = np.clip(layered_block_attention, low, high)\n",
    "    # print(layered_block_attention.shape)\n",
    "    block_attentions = layered_block_attention.transpose(0, 2, 1, 3).reshape(32 * total_size, 32 * total_size)\n",
    "\n",
    "    # plt.imsave(f'figures/{paragraph_index}.png', arr=block_attentions, cmap='viridis',format='png')\n",
    "\n",
    "    plt.imsave(f'figures/{paragraph_index}.png', arr=block_attentions, cmap='crest', format='png')\n",
    "\n",
    "    with open(f'figures/{paragraph_index}.txt', 'w') as f:\n",
    "        f.write(dataset[paragraph_index])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"1.png\", np.random.rand(5, 20), cmap='coolwarm', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([7,9,10,10.3,10.5])\n",
    "\n",
    "b = np.exp(a) / np.exp(a).sum()\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the colorbar\n",
    "# low -> high\n",
    "# -13 -> -4.5\n",
    "\n",
    "plt.imshow(block_attentions, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = templates.format(paragraph=dataset[975])\n",
    "input_ids = chat_tokenizer.tokenize_prompt(prompt, add_generation_prompt=False)\n",
    "separator_indices = [i for i, token in enumerate(input_ids[0]) if token in separator_tokens]\n",
    "\n",
    "for i in range(1, len(separator_indices)):\n",
    "    slice = input_ids[0][separator_indices[i-1]:separator_indices[i]]\n",
    "    print(repr(tokenizer.decode(slice[1:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
