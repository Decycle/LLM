{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-64g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "encoder_model_name = \"princeton-nlp/unsup-simcse-roberta-large\"\n",
    "encoder = AutoModel.from_pretrained(encoder_model_name).to(model.device)\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"Today my pet is sick and i am very sad. I plan to take her to the hospital tomorrow and see what the doctor can do. I hope it doesnt cost too much.\"\n",
    "with torch.no_grad():\n",
    "    target_embedding = encoder(**encoder_tokenizer(target_sentence, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)).pooler_output[0]\n",
    "\n",
    "target_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "prompt = \"Close your eyes, and tell me what do you see\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are very imaginative and creative, and you are picturing a scene in your mind. The scene is about \"\"\"{target_sentence}\"\"\"\n",
    "A ultra realistic scene begins to show up...<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "I see'''\n",
    "\n",
    "# output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "# print(tokenizer.decode(output[0]))\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored, cprint\n",
    "\n",
    "# cprint(\"Prompt:\", \"green\")\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "\n",
    "top_k = 20\n",
    "hint_weight = 5\n",
    "hint_weight_decay = 1.01\n",
    "# reptition_penalty = 0.98\n",
    "\n",
    "past_key_values = None\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "input_length = len(input_ids[0])\n",
    "ids_so_far = input_ids\n",
    "generation_mask = []\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(400):\n",
    "        output = model(input_ids=input_ids, return_dict=True, past_key_values=past_key_values)\n",
    "        logits = output.logits[0]\n",
    "        past_key_values = output.past_key_values\n",
    "\n",
    "        top_tokens = torch.topk(logits[-1], top_k).indices\n",
    "        raw_best_token = torch.argmax(logits[-1])\n",
    "\n",
    "        best_score = -10000\n",
    "        best_token = None\n",
    "\n",
    "        for top_token in top_tokens:\n",
    "            concat_indices = torch.cat([ids_so_far[0][input_length:], top_token.unsqueeze(0)])\n",
    "            string = tokenizer.decode(concat_indices)\n",
    "            encoder_tokens = encoder_tokenizer(string, return_tensors='pt').input_ids.cuda()\n",
    "            embedding = encoder(input_ids=encoder_tokens).pooler_output\n",
    "            similarity = cosine_similarity(target_embedding, embedding[0])\n",
    "\n",
    "            similarity = torch.max(similarity, torch.tensor(1e-5).to(model.device))\n",
    "\n",
    "            # seen_times = 0\n",
    "            # for token in ids_so_far[0][:-10]:\n",
    "            #     if token == top_token:\n",
    "            #         seen_times += 1\n",
    "\n",
    "            score = torch.log(similarity) * hint_weight + torch.log(logits[-1][top_token])\n",
    "            # score = score * (reptition_penalty ** seen_times)\n",
    "            # print(torch.log(similarity), torch.log(logits[-1][top_token]), score)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_token = top_token\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        if best_token == raw_best_token:\n",
    "            # print(\"Raw best token is the best token\")\n",
    "            generation_mask.append(1)\n",
    "            cprint(tokenizer.decode(best_token), end=\" \", color=\"green\")\n",
    "        else:\n",
    "            generation_mask.append(0)\n",
    "            cprint(tokenizer.decode(best_token), end=\" \", color=\"red\")\n",
    "\n",
    "        ids_so_far = torch.cat([ids_so_far, best_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "        input_ids = best_token.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        hint_weight *= hint_weight_decay\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            # print(tokenizer.decode(ids_so_far[0]), end=\" \")\n",
    "            print('\\n')\n",
    "            print('similarity:', similarity)\n",
    "            for i in range(len(generation_mask)):\n",
    "                if generation_mask[i] == 1:\n",
    "                    cprint(tokenizer.decode(ids_so_far[0][input_length + i]), end=\" \", color=\"green\")\n",
    "                else:\n",
    "                    cprint(tokenizer.decode(ids_so_far[0][input_length + i]), end=\" \", color=\"red\")\n",
    "            print('\\n\\n')\n",
    "        # break\n",
    "        # best_token = torch.argmax(logits[-1])\n",
    "        # ids_so_far = torch.cat([ids_so_far, best_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "        # input_ids = torch.cat([input_ids, best_token.unsqueeze(0).unsqueeze(0)], dim=-1)input_ids\n",
    "        # input_ids = best_token.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(ids_so_far[0][input_length:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "embeddings_array = torch.cat(embeddings).cpu().numpy()\n",
    "target_embeddings_cpu = target_embedding.cpu().numpy()\n",
    "\n",
    "pca.fit(embeddings_array)\n",
    "\n",
    "embeddings_pca = pca.transform(embeddings_array)\n",
    "target_embedding_pca = pca.transform(target_embeddings_cpu.reshape(1, -1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(embeddings_pca[:, 0], embeddings_pca[:, 1])\n",
    "plt.scatter(target_embedding_pca[:, 0], target_embedding_pca[:, 1], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(generation_mask)):\n",
    "    if generation_mask[i] == 1:\n",
    "        cprint(tokenizer.decode(ids_so_far[0][input_length + i]), end=\" \", color=\"green\")\n",
    "    else:\n",
    "        cprint(tokenizer.decode(ids_so_far[0][input_length + i]), end=\" \", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(f\"../dream/story{i}.txt\"):\n",
    "    i += 1\n",
    "\n",
    "with open(f\"../dream/story{i}.txt\", \"w\") as f:\n",
    "    f.write(f\"Hint: {target_sentence}\\n\\n\")\n",
    "    f.write(f\"Top k: {top_k}\\n\")\n",
    "    f.write(f\"Hint weight: {hint_weight}\\n\\n\")\n",
    "    f.write(tokenizer.decode(ids_so_far[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
