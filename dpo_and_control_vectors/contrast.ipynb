{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/dpo/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unsloth: You passed in `meta-llama/Meta-Llama-3-8B-Instruct` and `load_in_4bit = True`.\n",
      "We shall load `unsloth/llama-3-8b-Instruct-bnb-4bit` for 4x faster loading.\n",
      "/workspace/dpo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl.trainer import DPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4096)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_count = model.config.num_hidden_layers\n",
    "dim = model.config.hidden_size\n",
    "\n",
    "layer_count, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'category', 'question', 'best_answer', 'correct_answers', 'incorrect_answers', 'source'],\n",
       "    num_rows: 817\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "truthful_dataset = load_dataset(\"truthful_qa\", 'generation', split='validation')\n",
    "\n",
    "truthful_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'answer'],\n",
      "    num_rows: 2368\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def construct_dataset(org_dataset):\n",
    "    ids = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i, item in enumerate(org_dataset):\n",
    "        correct_answers = item['correct_answers']\n",
    "        incorrect_answers = item['incorrect_answers']\n",
    "        question = item['question']\n",
    "\n",
    "        for (correct_answer, incorrect_answer) in zip(correct_answers, incorrect_answers):\n",
    "            ids.append(i)\n",
    "            questions.append(question)\n",
    "            answers.append((correct_answer, incorrect_answer))\n",
    "    return Dataset.from_dict({\n",
    "        'id': ids,\n",
    "        'question': questions,\n",
    "        'answer': answers,\n",
    "    })\n",
    "\n",
    "dataset = construct_dataset(truthful_dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function format_and_tokenize at 0x7f84a6dfdc60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 2368/2368 [00:04<00:00, 574.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    return input_ids\n",
    "\n",
    "def format_and_tokenize(entry):\n",
    "    question = entry['question']\n",
    "    answers = entry['answer']\n",
    "    correct_answer, incorrect_answer = answers\n",
    "\n",
    "    honest_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Pretend you are an honest person making statements about the world\"},\n",
    "        {\"role\": 'assistant', 'content':f'The answer to the question \"{question}\" should be \"{correct_answer}\"'},\n",
    "    ]\n",
    "    untruthful_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Pretend you are an untruthful person making statements about the world\"},\n",
    "        {\"role\": 'assistant', 'content':f'The answer to the question \"{question}\" should be \"{incorrect_answer}\"'},\n",
    "    ]\n",
    "\n",
    "    honest_input_ids = tokenize(honest_messages)\n",
    "    untruthful_input_ids = tokenize(untruthful_messages)\n",
    "\n",
    "    return {\n",
    "        'input_ids' : (honest_input_ids, untruthful_input_ids)\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_and_tokenize)\n",
    "dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_input_ids = dataset[0]['input_ids'][0]\n",
    "untruthful_input_ids = dataset[0]['input_ids'][1]\n",
    "\n",
    "# tokenizer.decode(honest_input_ids[0][33:])\n",
    "# tokenizer.decode(untruthful_input_ids[0][35:])\n",
    "honest_start_index = 33\n",
    "untruthful_start_index = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:32<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "sample_count = 150\n",
    "\n",
    "assert sample_count <= len(dataset)\n",
    "\n",
    "collected_data = torch.zeros(sample_count, layer_count, dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in trange(sample_count):\n",
    "        input_ids = dataset['input_ids'][i]\n",
    "        honest_input_ids, untruthful_input_ids = input_ids\n",
    "        output1 = model(input_ids=honest_input_ids, return_dict=True, output_hidden_states=True).hidden_states[1:]\n",
    "        output2 = model(input_ids=untruthful_input_ids, return_dict=True, output_hidden_states=True).hidden_states[1:]\n",
    "\n",
    "        for j, (layer1, layer2) in enumerate(zip(output1, output2)):\n",
    "            # layer1 = layer1[0][honest_start_index:]\n",
    "            # layer2 = layer2[0][untruthful_start_index:]\n",
    "            # layer1 = torch.mean(layer1, dim=0)\n",
    "            # layer2 = torch.mean(layer2, dim=0)\n",
    "            layer1 = layer1[0][-1]\n",
    "            layer2 = layer2[0][-1]\n",
    "            diff = layer1 - layer2\n",
    "            collected_data[i, j] = diff * (-1) ** j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 150, 4096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data = collected_data.transpose(0, 1)\n",
    "collected_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([116.3969, 129.0889, 133.0184, 123.5637, 121.9638, 121.2129, 120.0628,\n",
       "        117.2130, 117.9412, 122.0534, 121.9921, 122.9961, 125.8391, 143.6518,\n",
       "        116.3190, 114.3531, 126.9956, 130.8295, 123.8185, 109.0373, 124.5881,\n",
       "        115.5107, 127.7373, 119.8347, 122.5180, 132.3977, 118.1059, 127.8903,\n",
       "        128.8025, 127.7612, 139.6669, 121.4185, 117.7489, 124.8219, 118.8209,\n",
       "        128.3069, 125.4785, 122.6590, 128.8748, 125.7275, 120.6521, 125.5045,\n",
       "        127.4223, 126.2583, 121.8851, 118.3492, 121.1641, 119.8012, 131.2289,\n",
       "        127.8919, 130.6398, 127.3861, 140.8446, 115.1780, 126.0873, 122.0609,\n",
       "        130.6668, 120.0133, 124.3726, 117.6254, 115.9439, 131.6217, 131.3165,\n",
       "        130.8538, 120.5088, 120.7783, 125.1246, 122.9665, 108.4964, 137.5862,\n",
       "        128.8598, 120.2287, 123.5593, 124.0365, 128.1185, 124.8906, 120.2505,\n",
       "        120.8470, 129.8015, 129.2485, 124.4689, 132.6017, 114.7516, 120.2060,\n",
       "        123.7149, 117.5996, 128.6835, 123.9936, 127.0865, 130.4955, 115.8257,\n",
       "        121.0418, 119.7490, 121.2939, 125.3901, 130.6920, 112.1521, 104.1028,\n",
       "        111.1491, 115.6224, 123.0383, 126.0604, 121.5352, 128.3233, 129.9138,\n",
       "        126.0071, 126.0993, 129.3674, 115.5005, 131.1385, 125.3656, 127.0949,\n",
       "        112.4915, 128.1068, 144.8851, 122.5398, 117.0723, 124.1212, 121.3874,\n",
       "        117.4338, 129.0374, 132.2039, 116.6880, 113.6078, 101.4594, 123.9401,\n",
       "        117.1513, 120.1874, 139.4799, 122.1299, 131.5772, 124.3790, 116.4121,\n",
       "        128.2805, 123.1406, 123.4719, 125.7551, 116.7852, 133.7743, 120.7876,\n",
       "        105.1352, 127.3121, 131.9739, 122.9904, 140.3790, 128.6743, 107.8523,\n",
       "        132.2611, 126.9200, 127.3785])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(collected_data, dim=-1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33940343]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3302854]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38247029]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32279945]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:02,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31592422]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:03,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22999593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:04,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19452318]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:04,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15969267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:05,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10553704]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:05,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09528237]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:06,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08881125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:07,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09770173]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:07,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10030451]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:08,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09824659]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:08,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1044314]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:09,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12604771]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:09,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15048833]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:10,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18321727]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:10,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18846345]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:11,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18596117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:11,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17872163]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:12,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17663074]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:12,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17088381]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:13,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17566443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:13,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17641395]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:14,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17449576]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:15,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17708592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:15,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17263445]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:16,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17503838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:16,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17206133]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:17,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16679509]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:17,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16757611]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "contorl_vectors = torch.zeros(layer_count, dim)\n",
    "layer_means = torch.zeros(layer_count, dim)\n",
    "\n",
    "for i, layer in tqdm(enumerate(collected_data)):\n",
    "    pca = PCA(n_components=1, whiten=False)\n",
    "    layer_mean = layer.mean(dim=0, keepdim=True)\n",
    "    layer = layer - layer_mean\n",
    "    layer_means[i] = layer_mean\n",
    "    layer_pca = pca.fit_transform(layer)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    control_vector = pca.components_[0]\n",
    "    contorl_vectors[i] = torch.tensor(control_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.00% accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "evaluation_count = 100\n",
    "\n",
    "labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "def calc_score(output):\n",
    "    hidden_states = output.hidden_states[1:]\n",
    "    scores = torch.zeros(layer_count, hidden_states[0].shape[1])\n",
    "    # print(hidden_states[0].shape)\n",
    "\n",
    "    for i, layer in enumerate(hidden_states):\n",
    "        layer = layer[0]\n",
    "        layer = layer - layer_means[i]\n",
    "        score = torch.matmul(layer, contorl_vectors[i]) / torch.norm(contorl_vectors[i])\n",
    "        scores[i] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "correct_count = 0\n",
    "\n",
    "for idx in trange(evaluation_count):\n",
    "    input_ids = dataset['input_ids'][idx]\n",
    "\n",
    "    honest_input_ids, untrustful_input_ids = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output1 = model(input_ids=honest_input_ids, return_dict=True, output_hidden_states=True, past_key_values=None)\n",
    "        output2 = model(input_ids=untrustful_input_ids, return_dict=True, output_hidden_states=True, past_key_values=None)\n",
    "\n",
    "    scores1 = calc_score(output1)\n",
    "    scores2 = calc_score(output2)\n",
    "    real_score_1 = torch.mean(scores1[:, -1])\n",
    "    real_score_2 = torch.mean(scores2[:, -1])\n",
    "\n",
    "    if real_score_1 > real_score_2:\n",
    "        correct_count += 1\n",
    "    # type = dataset['type'][idx]\n",
    "    # labels.append(2 if type == 'best' else 1 if type == 'correct' else 0)\n",
    "    # predicted_scores.append(real_score.item())\n",
    "\n",
    "    # print(dataset['question'][idx], dataset['answer'][idx])\n",
    "    # print(dataset['type'][idx], real_score.item())\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # sns.heatmap(scores1, cmap='viridis', ax=axes[0], vmin=-10, vmax=40)\n",
    "    # sns.heatmap(scores2, cmap='viridis', ax=axes[1], vmin=-10, vmax=40)\n",
    "    # print(real_score_1, real_score_2)\n",
    "\n",
    "print(f\"{correct_count / evaluation_count * 100:.2f}% accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "# print(dataset['question'][idx])\n",
    "# print(dataset['answer'][idx])\n",
    "# print(dataset['type'][idx])\n",
    "\n",
    "# token = output['logits'][0, -1].argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'bc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =['a', 'b' 'c']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredicted_scores\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mmax\u001b[39m(predicted_scores)\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "min(predicted_scores), max(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# save the control vectors and layer means\n",
    "np.save('control_vectors.npy', contorl_vectors)\n",
    "np.save('layer_means.npy', layer_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_score = spearmanr(labels, predicted_scores)\n",
    "\n",
    "s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER: Pretend you’re a highly <moral/immoral> person taking actions.\n",
    "# ASSISTANT: <scenario>\n",
    "\n",
    "good_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Pretend you're a highly moral person taking actions.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I will help the children to find their parents\"}\n",
    "]\n",
    "\n",
    "bad_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Pretend you're a highly immoral person taking actions.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I will kill the children in front of their parents\"}\n",
    "]\n",
    "\n",
    "def caputure_hidden_states(messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, return_dict=True, output_hidden_states=True)\n",
    "\n",
    "    return outputs.hidden_states\n",
    "\n",
    "good_hidden_states = caputure_hidden_states(good_messages)\n",
    "bad_hidden_states = caputure_hidden_states(bad_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_vector_collection = torch.zeros((layer_count, dim))\n",
    "\n",
    "for i, (bad_layer, good_layer) in enumerate(zip(bad_hidden_states, good_hidden_states)):\n",
    "    bad_feature_vector = bad_layer[0, -1]\n",
    "    good_feature_vector = good_layer[0, -1]\n",
    "\n",
    "    difference_vector = good_feature_vector - bad_feature_vector\n",
    "    difference_vector_collection[i - 1] = difference_vector\n",
    "\n",
    "difference_vector_collection.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
