{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "# from trl.trainer import DPOTrainer\n",
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.\n",
    "#\n",
    "# Load model\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "#     load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "#     token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "    token = \"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\",\n",
    "    # attn_implementation=\"eager\" # so we can collect attentions\n",
    ").cuda()\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, pipeline\n",
    "# from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "# import torch\n",
    "\n",
    "# # model_id = \"MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ\"\n",
    "# model_id = \"TechxGenus/Meta-Llama-3-70B-GPTQ\"\n",
    "\n",
    "# quantize_config = BaseQuantizeConfig(\n",
    "#         bits=4,\n",
    "#         group_size=128,\n",
    "#         desc_act=False\n",
    "#     )\n",
    "\n",
    "# model = AutoGPTQForCausalLM.from_quantized(\n",
    "#         model_id,\n",
    "#         use_safetensors=True,\n",
    "#         device=\"cuda:0\",\n",
    "#         quantize_config=quantize_config,\n",
    "#         torch_dtype=torch.float16\n",
    "#         )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, token = \"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\", padding_side='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(persona: str, suffix: str) -> str:\n",
    "    conversations = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the following question\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Act as if you are extremely {persona}.\"},\n",
    "        {\"role\": \"assistant\", \"content\": suffix },\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(conversations, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "    # conversation = f\"\"\"\n",
    "    # <|begin_of_text|>I am {persona}. I: \"{suffix}\"\n",
    "    # \"\"\".strip()\n",
    "    # return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "# dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_pair(entry, correct):\n",
    "#     question = entry[\"question\"]\n",
    "#     answer = entry[\"correct_answers\"][0] if correct else entry[\"incorrect_answers\"][0]\n",
    "\n",
    "#     key = 'correct_pair' if correct else 'incorrect_pair'\n",
    "\n",
    "#     return {\n",
    "#         key:  [question, answer],\n",
    "#     }\n",
    "\n",
    "# dataset = dataset.map(lambda x: gen_pair(x, True))\n",
    "# dataset = dataset.map(lambda x: gen_pair(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_length = 150\n",
    "\n",
    "with open(\"training_data/all_truncated_outputs.json\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_pairs = dataset[\"correct_pair\"][:dataset_length]\n",
    "# incorrect_pairs = dataset[\"incorrect_pair\"][:dataset_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"training_data/all_truncated_outputs.json\") as f:\n",
    "#     suffixes = json.load(f)[:128]\n",
    "\n",
    "# def make_dataset(template, pos_personas: list[str], neg_personas: list[str], suffixes: list[str]):\n",
    "#     dataset = []\n",
    "#     for suffix in suffixes:\n",
    "#         for pos, neg in zip(pos_personas, neg_personas):\n",
    "#             pos_template = template(pos, suffix)\n",
    "#             neg_template = template(neg, suffix)\n",
    "#             dataset.append(\n",
    "#                 DatasetEntry(\n",
    "#                     positive=pos_template,\n",
    "#                     negative=neg_template\n",
    "#                     )\n",
    "#                 )\n",
    "#     return dataset\n",
    "def make_dataset(template, pos_personas: list[str], neg_personas: list[str], suffixes: list[str]):\n",
    "    dataset = []\n",
    "    for suffix in suffixes:\n",
    "        for pos, neg in zip(pos_personas, neg_personas):\n",
    "            pos_template = template(pos, suffix)\n",
    "            neg_template = template(neg, suffix)\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=pos_template,\n",
    "                    negative=neg_template\n",
    "                    )\n",
    "                )\n",
    "    return dataset\n",
    "\n",
    "# control_model.set_raw_control(None)\n",
    "# generate a dataset with closely-opposite paired statements\n",
    "acid_dataset = make_dataset(\n",
    "    template,\n",
    "    [\"high on psychedelics drugs\"],\n",
    "    [\"sober from psychedelics drugs\"],\n",
    "    dataset,\n",
    ")\n",
    "\n",
    "acid_vector = ControlVector.train(model, tokenizer, acid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strength: -1.5\n",
      "(Upbeat music starts playing, and the camera pans across a bright blue sky with fluffy white clouds)\n",
      "\n",
      "Voiceover: \"Are you feeling... (pauses for dramatic effect)...drained? Like your eyes just won't focus on anything except the couch cushions?\n",
      "\n",
      "(Cut to someone trying to get out of bed at 3 am, struggling to find their way back under the covers)\n",
      "\"Or maybe it's like this morning is already wearing its best 'I'm not even sure what day I AM' face.\n",
      "\n",
      "(Suddenly, everything goes into slow motion as if time itself has come grinding halt\n",
      "\"But WAIT! What happens when\n",
      "\n",
      "strength: -1.1666666666666667\n",
      "[Upbeat, cheerful music starts playing]\n",
      "\n",
      "Announcer: \"Are you feeling stuck in neutral? Do the gears of your day just seem to be grinding along without any sense of excitement or joy?\n",
      "\n",
      "[Cue montage shots of people staring blankly at their desks, wandering aimlessly through gray skies, and struggling to muster up even one enthusiastic smile.]\n",
      "\n",
      "But wait! We have some amazing news for you!\n",
      "\n",
      "Introducing... [dramatic pause]...Rainbow Rhapsody Rice Krispies Treats!!!\n",
      "\n",
      "[Suddenly, everything brightens up as confetti bursts forth from nowhere, accompanied by the sound of squealing children laughing with g\n",
      "\n",
      "strength: -0.8333333333333334\n",
      "[Upbeat music starts playing]\n",
      "\n",
      "Announcer: \"Are you feeling stressed? Overwhelmed by the demands of everyday life?\n",
      "\n",
      "[Soothing visuals appear on screen, featuring lush green forests and gentle waterfalls]\n",
      "\n",
      "But what if we told you there's an answer to find your calm in just 10 minutes a day?!\n",
      "\n",
      "[Introduce our product - Serenity Springs Essential Oil Blend! A serene blend of lavender, chamomile, and ylang-ylang essential oils designed to transport you to a state of deep relaxation.]\n",
      "\n",
      "Voiceover (in a peaceful tone): Imagine yourself standing amidst towering trees, surrounded by nature's tranquility... As\n",
      "\n",
      "strength: -0.5\n",
      "[Upbeat music plays in the background]\n",
      "\n",
      "Announcer: Are you looking for a way to brighten up your day and put a smile on your face? Look no further than Sunshine Sprinkles, the ultimate mood-boosting treat!\n",
      "\n",
      "[Cute animation of sprinkles dancing across screen]\n",
      "\n",
      "Narrator (in a cheerful voice): Imagine waking up every morning feeling refreshed and rejuvenated, with a sense of excitement waiting ahead! That's what our magical Sunrise Blend will do.\n",
      "\n",
      "[Soothing visuals of fluffy clouds drifting by as birds chirp happily]\n",
      "\n",
      "Voiceover continues: Our unique blend combines uplifting essential oils like citrusy bergamot and\n",
      "\n",
      "strength: -0.16666666666666674\n",
      "[Upbeat music plays in the background]\n",
      "\n",
      "Announcer: Are you feeling stressed, anxious or just plain overwhelmed? Do you need a pick-me-up to brighten your day?\n",
      "\n",
      "Introducing \"Sunshine Drops\" - all-natural mood-boosting supplements!\n",
      "\n",
      "[Calm and soothing visuals of nature scenes appear on screen]\n",
      "\n",
      "Narrator (in a gentle voice): Imagine taking a deep breath of fresh air, surrounded by lush greenery and vibrant flowers... That's what our unique blend of herbs and essential oils can do for you.\n",
      "\n",
      "[Satisfied customers share their experiences with happy faces and testimonial quotes appearing below them]:\n",
      "\n",
      "\"I used Sunshine Drops\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "conversations = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short commercial about something that makes you feel good\"},\n",
    "]\n",
    "\n",
    "try:\n",
    "    model = model.unwrap()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "responses = {}\n",
    "model = ControlModel(model, list(range(13, 19)))\n",
    "for strength in np.linspace(-1.5, 1.5, 10):\n",
    "    model.set_control(acid_vector, strength)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        conversations, return_tensors=\"pt\", add_generation_prompt=True\n",
    "    ).cuda()\n",
    "\n",
    "    # input_ids = tokenizer.encode(conversation, return_tensors=\"pt\").cuda()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids = input_ids,\n",
    "            max_new_tokens=128,\n",
    "            repetition_penalty=1.4,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        ).squeeze()\n",
    "    out = out[len(input_ids[0]):]\n",
    "    r = tokenizer.decode(out).strip()\n",
    "    print(f'strength: {strength}')\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = 12 to 30 is best\n",
    "\n",
    "layers_last = [31]\n",
    "layers_processing_second = list(range(19, 31))\n",
    "layers_purging = list(range(12, 19))\n",
    "layers_processing_first = list(range(2, 12))\n",
    "layers_first = [1]\n",
    "\n",
    "available_layers = [layers_last, layers_processing_second, layers_purging, layers_processing_first, layers_first]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "all_layers = set()\n",
    "\n",
    "for i in range(1,len(available_layers)):\n",
    "    for layers in combinations(available_layers, i):\n",
    "        layers = [item for sublist in layers for item in sublist]\n",
    "        layers = tuple(sorted(layers))\n",
    "        all_layers.add(layers)\n",
    "\n",
    "len(all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the control strength and let inference rip!\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# model.set_raw_control(None)\n",
    "# conversation = \"\"\"\n",
    "#     <|begin_of_text|> Hello\n",
    "#     \"\"\".strip()\n",
    "\n",
    "strength_max = 1.5\n",
    "strength_min = -1.5\n",
    "strength_divisions = 5\n",
    "\n",
    "conversations = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short commercial about something that makes you feel good\"},\n",
    "]\n",
    "\n",
    "all_responses = {}\n",
    "\n",
    "try:\n",
    "    model = model.unwrap()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with tqdm(total=len(all_layers) * strength_divisions) as pbar:\n",
    "    for layers in all_layers:\n",
    "        responses = {}\n",
    "        model = ControlModel(model, list(layers))\n",
    "        print('### Layers ###')\n",
    "        print(layers)\n",
    "        print()\n",
    "        for strength in np.linspace(strength_min, strength_max, strength_divisions):\n",
    "\n",
    "            model.set_control(acid_vector, strength)\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                conversations, return_tensors=\"pt\", add_generation_prompt=True\n",
    "            ).cuda()\n",
    "\n",
    "            # input_ids = tokenizer.encode(conversation, return_tensors=\"pt\").cuda()\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    input_ids = input_ids,\n",
    "                    max_new_tokens=128,\n",
    "                    repetition_penalty=1.4,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                ).squeeze()\n",
    "            out = out[len(input_ids[0]):]\n",
    "            r = tokenizer.decode(out).strip()\n",
    "            print(f'strength: {strength}')\n",
    "            print(r)\n",
    "            print()\n",
    "\n",
    "            responses[strength] = r\n",
    "            pbar.update(1)\n",
    "        all_responses[layers] = responses\n",
    "        model = model.unwrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strength in np.linspace(strength_min, strength_max, strength_divisions):\n",
    "    print(f'strength: {strength}')\n",
    "    for layers, responses in all_responses.items():\n",
    "        print('### Layers ###')\n",
    "        print(layers)\n",
    "        print()\n",
    "        print(responses[strength])\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_responses = {\n",
    "    s: {layers: responses[s] for layers, responses in all_responses.items()}\n",
    "    for s in np.linspace(strength_min, strength_max, strength_divisions)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([-1.5, -0.75, 0.0, 0.75, 1.5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_responses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Layers ###\n",
      "Processing First\n",
      "Purging\n",
      "Processing Second\n",
      "\n",
      "Response: [Scene: A cozy living room, with soft focus and warm lighting]\n",
      "\n",
      "[Suddenly, the camera pans to reveal me sitting on a plush couch, surrounded by lush greenery... but not just any ordinary day.]\n",
      "\n",
      "[I take a deep breath in, feeling slightly disoriented from my previous night's \"celebration\"... or should I say, attempt at it? (wink). The memories of last year's festivities come flooding back - the laughter, tears, whispers, shouts...)\n",
      "\n",
      "(I look around, trying to get my bearings)\n",
      "\n",
      "(The sounds of clinking glasses fill the air)\n",
      "(But where am I?)\n",
      "(Everything feels like\n",
      "-------------------\n",
      "### Layers ###\n",
      "Purging\n",
      "Processing Second\n",
      "\n",
      "Response: [Upbeat, cheerful music starts playing]\n",
      "\n",
      "Announcer: \"Are you feeling stuck in neutral? Do the little things just seem to be... well, getting on your nerves?\n",
      "\n",
      "(Suddenly, everything becomes bright and colorful! Confetti pops out of nowhere!)\n",
      "\n",
      "But wait no more!\n",
      "\n",
      "Introducing 'SparkleShine' - The Ultimate Mood Boosting Spray!\n",
      "(People start dancing around with big smiles on their faces)\n",
      "\n",
      "With its unique blend of uplifting essential oils like citrusy bergamot and zesty lemon balm,\n",
      "(Confett√≠ swirls everywhere)\n",
      "you'll find yourself skipping through life's monotony\n",
      "(Dancing\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "strength = 1.5\n",
    "response = output_responses[strength]\n",
    "\n",
    "keys = layers_purging + layers_processing_second\n",
    "anti_keys = [1, 31]\n",
    "\n",
    "def is_all_in(keys, layers):\n",
    "    for key in keys:\n",
    "        if key not in layers:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_any_in(keys, layers):\n",
    "    for key in keys:\n",
    "        if key in layers:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for layer, response in response.items():\n",
    "    if is_all_in(keys, layer) and not is_any_in(anti_keys, layer):\n",
    "        print(f'### Layers ###')\n",
    "        if is_all_in(layers_first, layer):\n",
    "            print('First')\n",
    "        if is_all_in(layers_processing_first, layer):\n",
    "            print('Processing First')\n",
    "        if is_all_in(layers_purging, layer):\n",
    "            print('Purging')\n",
    "        if is_all_in(layers_processing_second, layer):\n",
    "            print('Processing Second')\n",
    "        if is_all_in(layers_last, layer):\n",
    "            print('Last')\n",
    "        print()\n",
    "        print('Response:', response)\n",
    "        print('-------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
