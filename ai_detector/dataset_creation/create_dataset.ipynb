{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas python-dotenv openai tenacity tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import openai\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file, ignore decoding errors\n",
    "with open('News.csv', \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'subtitle', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3824,\n",
       "                                                title subtitle  \\\n",
       " 0  Betsy DeVos Confirmed as Education Secretary, ...      NaN   \n",
       " 1  Melania Trump Says White House Could Mean Mill...      NaN   \n",
       " 2  As Trump Fears Fraud, GOP Eliminates Election ...      NaN   \n",
       " 3  Appeals Court to Decide on Challenge to Trump'...      NaN   \n",
       " 4  At Least 4 Tornadoes Reported in Southeast Lou...      NaN   \n",
       " \n",
       "                                                 text  \n",
       " 0  Michigan billionaire education activist Betsy ...  \n",
       " 1  First lady Melania Trump has said little about...  \n",
       " 2  A House committee voted on Tuesday to eliminat...  \n",
       " 3  This afternoon, three federal judges from the ...  \n",
       " 4  At least four tornadoes touched down in Louisi...  )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dataset_len = 100\n",
    "max_wait_time = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Human Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    subtitle = row['subtitle']\n",
    "    text = row['text']\n",
    "\n",
    "    if not pd.isnull(subtitle):\n",
    "        article = f'{title}\\n{subtitle}\\n{text}'\n",
    "    else:\n",
    "        article = f'{title}\\n{text}'\n",
    "\n",
    "    human_dataset.append({\"article\": article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('human_dataset.json', 'w') as f:\n",
    "    json.dump(human_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Generation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompts = []\n",
    "\n",
    "generation_prompt_template_subtitle = \"\"\"\n",
    "Write a news article about the following topic:\n",
    "Title: {title}\n",
    "Subtitle: {subtitle}\n",
    "Only output the written news article.\n",
    "\"\"\"\n",
    "\n",
    "generation_prompt_template_no_subtitle = \"\"\"\n",
    "Write a news article about the following topic:\n",
    "Title: {title}\n",
    "Only output the written news article.\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    subtitle = row['subtitle']\n",
    "\n",
    "    if subtitle != np.nan:\n",
    "        generation_prompt = generation_prompt_template_subtitle.format(title=title, subtitle=subtitle)\n",
    "    else:\n",
    "        generation_prompt = generation_prompt_template_no_subtitle.format(title=title)\n",
    "\n",
    "    generation_prompts.append(generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "def after_retry_callback(retry_state):\n",
    "    print(f\"Retry attempt: {retry_state.attempt_number}. Waiting for next attempt...\")\n",
    "\n",
    "async def generate(prompt):\n",
    "    try:\n",
    "        # Setting a timeout of 1 minute\n",
    "        return await asyncio.wait_for(\n",
    "            openai.ChatCompletion.acreate(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            ),\n",
    "            timeout=max_wait_time  # Set your desired timeout in seconds here\n",
    "        )\n",
    "    except asyncio.TimeoutError:\n",
    "        # Handle the timeout, e.g., return an error message or log it\n",
    "        return {'error': 'timeout', 'prompt': prompt}\n",
    "\n",
    "def get_text(output):\n",
    "    return output['choices'][0]['message']['content']\n",
    "\n",
    "async def generate_prompts(prompts):\n",
    "    tasks = [\n",
    "        generate(prompt) for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    success_indices = []\n",
    "    index = 0\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc='Tasks', unit='task', leave=False, ncols=100):\n",
    "        try:\n",
    "            result = await future\n",
    "            if result is None:\n",
    "                raise Exception('Timeout')\n",
    "            results.append({\n",
    "                'article': get_text(result)\n",
    "            })\n",
    "            success_indices.append(index)\n",
    "        except Exception as e:\n",
    "            print(f\"A task failed with error: {e}\")  # Handle error (if needed)\n",
    "\n",
    "    return results, success_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoawait\n",
    "from aiohttp import ClientSession\n",
    "openai.aiosession.set(ClientSession())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_dataset, _ = await generate_prompts(generation_prompts[:export_dataset_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Refinement Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "refinement_prompts = []\n",
    "\n",
    "refinement_prompt_template_subtitle = \"\"\"\n",
    "For the following news article, refine the article to be more concise and professional. Be entertaining, informative and engaging.\n",
    "Title: {title}\n",
    "Subtitle: {subtitle}\n",
    "Article: {text}\n",
    "Only output the written news article.\n",
    "\"\"\"\n",
    "\n",
    "refinement_prompt_template_no_subtitle = \"\"\"\n",
    "For the following news article, refine the article to be more concise and professional. Be entertaining, informative and engaging.\n",
    "Title: {title}\n",
    "Article: {text}\n",
    "Only output the written news article.\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    subtitle = row['subtitle']\n",
    "    text = row['text']\n",
    "\n",
    "    if subtitle != np.nan:\n",
    "        refinement_prompt = refinement_prompt_template_subtitle.format(title=title, subtitle=subtitle, text=text)\n",
    "    else:\n",
    "        refinement_prompt = refinement_prompt_template_no_subtitle.format(title=title, text=text)\n",
    "\n",
    "    refinement_prompts.append(refinement_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_wait_time = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tasks:   0%|                                                              | 0/100 [00:00<?, ?task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tasks:   1%|▌                                                     | 1/100 [00:01<01:52,  1.13s/task]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A task failed with error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4206 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tasks:  98%|███████████████████████████████████████████████████▉ | 98/100 [00:20<00:03,  1.55s/task]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7f279ecbb9a0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7f279df99e40>\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A task failed with error: 'choices'\n",
      "A task failed with error: 'choices'\n",
      "Batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "refinement_datasets = []\n",
    "for i in range(0, 200, batch_size):\n",
    "    print(f'Batch {i // batch_size + 1}')\n",
    "    refinement_dataset, _ = await generate_prompts(refinement_prompts[i:i + batch_size])\n",
    "    refinement_datasets.extend(refinement_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refinement_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('refinement_dataset.json', 'w') as f:\n",
    "    json.dump(refinement_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Completion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_prompts = []\n",
    "completion_masks = []\n",
    "\n",
    "completion_prompt_template_subtitle = \"\"\"\n",
    "Finish the following uncomplete news article. Be entertaining, informative and engaging, and make sure the tone is consistent with the previous article.\n",
    "Title: {title}\n",
    "Subtitle: {subtitle}\n",
    "Article: {text}\n",
    "Only output the written news article.\n",
    "\"\"\"\n",
    "\n",
    "completion_prompt_template_no_subtitle = \"\"\"\n",
    "Finish the following uncomplete news article. Be entertaining, informative and engaging, and make sure the tone is consistent with the previous article.\n",
    "Title: {title}\n",
    "Article: {text}\n",
    "Only output the written news article.\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    subtitle = row['subtitle']\n",
    "    text = row['text']\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "\n",
    "    text = text[:(len(text) // 2)]\n",
    "\n",
    "    completion_masks.append(len(text))\n",
    "\n",
    "    if subtitle != np.nan:\n",
    "        completion_prompt = completion_prompt_template_subtitle.format(title=title, subtitle=subtitle, text=text)\n",
    "    else:\n",
    "        completion_prompt = completion_prompt_template_no_subtitle.format(title=title, text=text)\n",
    "\n",
    "    completion_prompts.append(completion_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A task failed with error: 'choices'\n",
      "Batch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A task failed with error: 'choices'\n",
      "Batch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(refinement_prompts), batch_size):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mbatch_size\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     completion_dataset, success_indicies \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m generate_prompts(completion_prompts[i:i \u001b[39m+\u001b[39m batch_size])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     refinement_datasets\u001b[39m.\u001b[39mextend(refinement_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     success_masks\u001b[39m.\u001b[39mextend([completion_masks[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m success_indicies])\n",
      "\u001b[1;32m/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m tqdm(asyncio\u001b[39m.\u001b[39mas_completed(tasks), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(tasks), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTasks\u001b[39m\u001b[39m'\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, ncols\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m future\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2BUbuntu@tunnel%2Bdecycle-pc/home/decycle/ML/fine-tune-experiments/huggingface/dataset_creation/create_dataset.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mTimeout\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/asyncio/tasks.py:567\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_wait_for_one\u001b[39m():\n\u001b[0;32m--> 567\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m done\u001b[39m.\u001b[39mget()\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         \u001b[39m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    570\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mTimeoutError\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/asyncio/queues.py:159\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getters\u001b[39m.\u001b[39mappend(getter)\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mawait\u001b[39;00m getter\n\u001b[1;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     getter\u001b[39m.\u001b[39mcancel()  \u001b[39m# Just in case getter is not done yet.\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "batch_size = 100\n",
    "max_wait_time = 60\n",
    "\n",
    "completion_datasets = []\n",
    "success_masks = []\n",
    "for i in range(0, len(refinement_prompts), batch_size):\n",
    "    print(f'Batch {i // batch_size + 1}')\n",
    "    completion_dataset, success_indicies = await generate_prompts(completion_prompts[i:i + batch_size])\n",
    "    refinement_datasets.extend(refinement_dataset)\n",
    "    success_masks.extend([completion_masks[i] for i in success_indicies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_dataset = [\n",
    "    {\n",
    "        'article': data['article'],\n",
    "        'mask': mask\n",
    "    }\n",
    "    for data, mask in zip(completion_dataset, completion_masks)\n",
    "]\n",
    "\n",
    "with open('completion_dataset.json', 'w') as f:\n",
    "    json.dump(completion_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated dataset\")\n",
    "\n",
    "for d in generation_dataset[:10]:\n",
    "    print(d['article'][:100])\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"Refinement dataset\")\n",
    "\n",
    "for d in refinement_dataset[:10]:\n",
    "    print(d['article'][:100])\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"Completion dataset\")\n",
    "\n",
    "for d in completion_dataset[:10]:\n",
    "    print(d['article'][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(generation_dataset), len(refinement_dataset), len(completion_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
