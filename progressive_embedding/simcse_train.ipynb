{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Import our models. The package will take care of downloading the models automatically\n",
    "\n",
    "model_id = \"princeton-nlp/unsup-simcse-roberta-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained('/workspace/model').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "model.eval()\n",
    "\n",
    "def evaluate(texts):\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    for i, text in enumerate(texts):\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**input_ids).last_hidden_state[0]\n",
    "            print(output.shape)\n",
    "            outputs.append(output.cpu().numpy())\n",
    "            labels.append(np.ones(output.shape[0]) * i)\n",
    "\n",
    "    np_outputs = np.concatenate(outputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    outputs_2d = pca.fit_transform(np_outputs)\n",
    "\n",
    "    min_x, max_x = outputs_2d[:, 0].min(), outputs_2d[:, 0].max()\n",
    "    min_y, max_y = outputs_2d[:, 1].min(), outputs_2d[:, 1].max()\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(texts), figsize=(20, 10))\n",
    "\n",
    "    current_start_idx = 0\n",
    "    for i, text in enumerate(texts):\n",
    "        axes[i].set_xlim(min_x, max_x)\n",
    "        axes[i].set_ylim(min_y, max_y)\n",
    "        axes[i].set_title(text)\n",
    "        current_len = outputs[i].shape[0]\n",
    "        axes[i].scatter(outputs_2d[current_start_idx:current_start_idx + current_len, 0], outputs_2d[current_start_idx:current_start_idx + current_len, 1])\n",
    "        axes[i].plot(outputs_2d[current_start_idx:current_start_idx + current_len, 0], outputs_2d[current_start_idx:current_start_idx + current_len, 1])\n",
    "        axes[i].set_xlim(min_x - 2, max_x + 2)\n",
    "        axes[i].set_ylim(min_y - 2, max_y + 2)\n",
    "\n",
    "        current_start_idx += current_len\n",
    "    plt.show()\n",
    "\n",
    "evaluate([\n",
    "    \"I like to eat apples\",\n",
    "    \"I hate to eat bananas\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPCOH = 1\n",
    "LR = 1e-5\n",
    "TEMPERATURE = 0.05\n",
    "MAX_LEN = 512\n",
    "VALIDATION_SIZE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:10000]\")\n",
    "\n",
    "len(dataset)\n",
    "\n",
    "# shuffle the dataset\n",
    "dataset = dataset.shuffle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_MULTIPLIER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_batch_size = BATCH_SIZE * TOKENIZER_BATCH_MULTIPLIER\n",
    "token_num_batches = len(dataset) // token_batch_size\n",
    "# num_batches = 100\n",
    "\n",
    "def get_batch(dataset):\n",
    "\n",
    "    input_ids = torch.zeros((token_num_batches, token_batch_size, MAX_LEN), dtype=torch.long)\n",
    "    attention_masks = torch.zeros((token_num_batches, token_batch_size, MAX_LEN), dtype=torch.long)\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), token_batch_size)):\n",
    "        if i // token_batch_size >= token_num_batches:\n",
    "            break\n",
    "        data = dataset[i:i+token_batch_size]\n",
    "        data = data['text']\n",
    "        data = tokenizer(data, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        input_ids[i//token_batch_size] = data['input_ids']\n",
    "        attention_masks[i//token_batch_size] = data['attention_mask']\n",
    "\n",
    "    print(input_ids.shape)\n",
    "    input_ids = input_ids.reshape((-1, BATCH_SIZE, MAX_LEN))\n",
    "    attention_masks = attention_masks.reshape((-1, BATCH_SIZE, MAX_LEN))\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = get_batch(dataset)\n",
    "num_batches = input_ids.shape[0]\n",
    "# save\n",
    "torch.save(input_ids, '/workspace/data/all_512/input_ids.pt')\n",
    "torch.save(attention_masks, '/workspace/data/all_512/attention_masks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.load('/workspace/data/all_512/input_ids.pt')\n",
    "attention_masks = torch.load('/workspace/data/all_512/attention_masks.pt')\n",
    "\n",
    "num_batches = input_ids.shape[0]\n",
    "\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average size\n",
    "torch.sum(attention_masks) / (attention_masks.shape[0] * attention_masks.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"simcse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def log_graph(model, step):\n",
    "    input_string = \"\"\"\n",
    "    The reason for the high price of oil is the war in the Middle East. As the war continues, the price of oil will continue to rise. Authorities say that the price of oil will reach $100 per barrel. The price of oil has already reached $80 per barrel.\n",
    "    \"\"\"\n",
    "\n",
    "    second_string = \"\"\"\n",
    "    do you know why I like you? Because you are this shining star in my life. You are the one who makes me happy. You are the one who makes me feel good. You are the one who makes me feel special. You are the one who makes me fee\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").to(device).input_ids\n",
    "    input_ids2 = tokenizer(second_string, return_tensors=\"pt\").to(device).input_ids\n",
    "    with torch.no_grad():\n",
    "        output1 = model(input_ids).last_hidden_state[0].detach().cpu().numpy()\n",
    "        output2 = model(input_ids).last_hidden_state[0].detach().cpu().numpy()\n",
    "        output3 = model(input_ids2).last_hidden_state[0].detach().cpu().numpy()\n",
    "\n",
    "    outputs = np.concatenate([output1, output2, output3])\n",
    "    labels = np.concatenate([\n",
    "        np.zeros(output1.shape[0]),\n",
    "        np.ones(output2.shape[0]) * 0.5,\n",
    "        np.ones(output3.shape[0]),\n",
    "    ])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    outputs_2d = pca.fit_transform(outputs)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels)\n",
    "\n",
    "    output_2d = pca.fit_transform(output1)\n",
    "    axes[1].scatter(output_2d[:, 0], output_2d[:, 1], c=np.arange(output_2d.shape[0]))\n",
    "\n",
    "    wandb.log({\n",
    "        'graph': wandb.Image(fig),\n",
    "    })\n",
    "\n",
    "    # close\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.99\n",
    "eps_array = torch.pow(eps, torch.arange(MAX_LEN))\n",
    "eps_array = 1 - eps_array\n",
    "eps_array = eps_array.unsqueeze(0).to(device)\n",
    "\n",
    "eps_array.shape\n",
    "# eps_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download snli dataset\n",
    "from torch.nn import CosineSimilarity\n",
    "from scipy.stats import spearmanr\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stsb_multi_mt\", 'en', split=\"test\").with_format(\n",
    "    \"torch\", device=device)\n",
    "\n",
    "def prepare_validation_set(dataset, max_size=VALIDATION_SIZE):\n",
    "    batch = []\n",
    "    for i in range(max_size * BATCH_SIZE):\n",
    "        data = dataset[i]\n",
    "        premise = tokenizer(data['sentence1'], padding='max_length', truncation=True,\n",
    "                            return_tensors='pt', max_length=MAX_LEN)\n",
    "        hypothesis = tokenizer(data['sentence2'], padding='max_length', truncation=True,\n",
    "                               return_tensors='pt', max_length=MAX_LEN)\n",
    "        batch.append((premise, hypothesis, data['similarity_score']))\n",
    "\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            token_ids = torch.zeros(2, BATCH_SIZE, MAX_LEN, dtype=torch.long)\n",
    "            token_masks = torch.zeros(2, BATCH_SIZE, MAX_LEN, dtype=torch.long)\n",
    "            scores = torch.zeros(BATCH_SIZE, dtype=torch.float)\n",
    "\n",
    "            for i, (premise, hypothesis, score) in enumerate(batch):\n",
    "                token_ids[0, i] = premise['input_ids']\n",
    "                token_ids[1, i] = hypothesis['input_ids']\n",
    "\n",
    "                token_masks[0, i] = premise['attention_mask']\n",
    "                token_masks[1, i] = hypothesis['attention_mask']\n",
    "\n",
    "                scores[i] = score\n",
    "\n",
    "            yield token_ids, token_masks, scores\n",
    "            batch = []\n",
    "\n",
    "cos = CosineSimilarity()\n",
    "validation_set = list(prepare_validation_set(dataset))\n",
    "\n",
    "def validate(model, step=0, log=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicated_empty_scores = []\n",
    "        predicated_last_scores = []\n",
    "        predicated_mutual_last_scores = []\n",
    "        actual_scores = []\n",
    "        for batch in validation_set:\n",
    "            token_ids, token_masks, scores = batch\n",
    "            token_ids = token_ids.to(device)\n",
    "            token_masks = token_masks.to(device)\n",
    "            scores = scores.to(device)\n",
    "\n",
    "            sentence1_outputs = model(\n",
    "                input_ids=token_ids[0], attention_mask=token_masks[0]).last_hidden_state\n",
    "\n",
    "            sentence2_outputs = model(\n",
    "                input_ids=token_ids[1], attention_mask=token_masks[1]).last_hidden_state\n",
    "\n",
    "            empty_sentence1_output = sentence1_outputs[:, -1]\n",
    "            empty_sentence2_output = sentence2_outputs[:, -1]\n",
    "\n",
    "            predicated_empty_score = cos(empty_sentence1_output, empty_sentence2_output)\n",
    "            predicated_empty_scores.extend(predicated_empty_score.cpu().numpy())\n",
    "\n",
    "            length1 = token_masks[0].sum(dim=1)\n",
    "            length2 = token_masks[1].sum(dim=1)\n",
    "\n",
    "            last_sentence1_output = sentence1_outputs[torch.arange(sentence1_outputs.shape[0]), length1 - 1]\n",
    "            last_sentence2_output = sentence2_outputs[torch.arange(sentence2_outputs.shape[0]), length2 - 1]\n",
    "\n",
    "            predicated_last_score = cos(last_sentence1_output, last_sentence2_output)\n",
    "            predicated_last_scores.extend(predicated_last_score.cpu().numpy())\n",
    "\n",
    "            shortest_length = torch.min(length1, length2)\n",
    "\n",
    "            mutual_last_sentence1_output = sentence1_outputs[torch.arange(sentence1_outputs.shape[0]), shortest_length - 1]\n",
    "            mutual_last_sentence2_output = sentence2_outputs[torch.arange(sentence2_outputs.shape[0]), shortest_length - 1]\n",
    "\n",
    "            predicated_mutual_last_score = cos(mutual_last_sentence1_output, mutual_last_sentence2_output)\n",
    "            predicated_mutual_last_scores.extend(predicated_mutual_last_score.cpu().numpy())\n",
    "\n",
    "            actual_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "\n",
    "        rank_empty = spearmanr(predicated_empty_scores, actual_scores)\n",
    "        rank_last = spearmanr(predicated_last_scores, actual_scores)\n",
    "        rank_mutual_last = spearmanr(predicated_mutual_last_scores, actual_scores)\n",
    "    model.train()\n",
    "    if log:\n",
    "        wandb.log({\n",
    "            'rank_empty': rank_empty.correlation,\n",
    "            'rank_last': rank_last.correlation,\n",
    "            'rank_mutual_last': rank_mutual_last.correlation,\n",
    "        })\n",
    "    return rank_empty.correlation, rank_last.correlation, rank_mutual_last.correlation\n",
    "validate(model, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "cuda.empty_cache()\n",
    "model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from random import randint\n",
    "\n",
    "model.train()\n",
    "\n",
    "# critertion = CrossEntropyLoss(reduction='none')\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPCOH):\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        input_id = input_ids[i]\n",
    "        attention_mask = attention_masks[i]\n",
    "\n",
    "        embeddings_1 = model(input_id, attention_mask).last_hidden_state\n",
    "        embeddings_2 = model(input_id, attention_mask).last_hidden_state\n",
    "\n",
    "        embeddings_1 = embeddings_1 / torch.norm(embeddings_1, dim=-1, keepdim=True)\n",
    "        embeddings_2 = embeddings_2 / torch.norm(embeddings_2, dim=-1, keepdim=True)\n",
    "\n",
    "        embeddings_1 = embeddings_1 * attention_mask.unsqueeze(-1)\n",
    "        embeddings_2 = embeddings_2 * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        embeddings_1 = embeddings_1.reshape(-1, 1024)\n",
    "        embeddings_2 = embeddings_2.reshape(-1, 1024)\n",
    "\n",
    "        similarity = torch.mm(embeddings_1, embeddings_2.T)\n",
    "        similarity = similarity.reshape(BATCH_SIZE, MAX_LEN, BATCH_SIZE, MAX_LEN)\n",
    "\n",
    "        similarity = torch.exp(similarity / 0.05)\n",
    "\n",
    "        similarity = torch.sum(similarity, dim=(1, 3)) / torch.sum(attention_mask, dim=-1)\n",
    "\n",
    "        chosen_sim = torch.diagonal(similarity)\n",
    "        sum_sim = torch.sum(similarity, dim=-1)\n",
    "\n",
    "        loss = -torch.log(chosen_sim / sum_sim)\n",
    "        loss = torch.mean(loss)\n",
    "        # with torch.no_grad():\n",
    "        # batch = {\n",
    "        #     'input_ids': input_ids[i],\n",
    "        #     'attention_mask': attention_masks[i]\n",
    "        # }\n",
    "        # output1_raw = model(**batch).last_hidden_state\n",
    "        # output2_raw = model(**batch).last_hidden_state\n",
    "\n",
    "        # length = torch.min(torch.sum(attention_masks[i], dim=1))\n",
    "\n",
    "        # output1 = output1_raw[:, :length]\n",
    "        # output2 = output2_raw[:, :length]\n",
    "\n",
    "        # masked the output by setting padding tokens to 0\n",
    "        # output1_masked = output1_raw * attention_masks[i].unsqueeze(-1)\n",
    "        # output2_masked = output2_raw * attention_masks[i].unsqueeze(-1)\n",
    "\n",
    "        # # get the index of last token of each sentence\n",
    "        # last_zero_indices = torch.sum(attention_masks[0], dim=-1) - 1\n",
    "        # # make sure no index is less than 0\n",
    "        # last_zero_indices[last_zero_indices == -1] = 0\n",
    "        # # prepare for gather\n",
    "        # last_zero_indices = last_zero_indices.unsqueeze(1).unsqueeze(-1).repeat(1, 1, 1024)\n",
    "\n",
    "        # # get the last token of each sentence\n",
    "        # last_output1 = torch.gather(output1_masked, 1, last_zero_indices).repeat(1, MAX_LEN, 1)\n",
    "        # # expand and max to fill the padding token's space\n",
    "        # last_output1_masked = last_output1 * (1 - attention_masks[i]).unsqueeze(-1)\n",
    "        # # add to duplicate last item of each row to fill the padding token's space\n",
    "        # output1 = output1_masked + last_output1_masked\n",
    "\n",
    "        # last_output2 = torch.gather(output2_masked, 1, last_zero_indices).repeat(1, MAX_LEN, 1)\n",
    "        # last_output2_masked = last_output2 * (1 - attention_masks[i]).unsqueeze(-1)\n",
    "        # output2 = output2_masked + last_output2_masked\n",
    "\n",
    "        # print(last_zero_indices.shape)\n",
    "        # print(attention_masks[0][1][last_zero_indices[1, 0]])\n",
    "        # print(torch.gather(attention_masks[0], 1, last_zero_indices))\n",
    "        # print(torch.gather(attention_masks[0], 1, last_zero_indices.unsqueeze(0))[:, 0])\n",
    "\n",
    "        # norm1 = torch.norm(output1, dim=2, keepdim=True)\n",
    "        # norm2 = torch.norm(output2, dim=2, keepdim=True)\n",
    "\n",
    "        # # normalize\n",
    "        # output1_norm = output1 / norm1\n",
    "        # output2_norm = output2 / norm2\n",
    "\n",
    "        # transpose to prepare for batch matrix multiplication\n",
    "        # output1_transposed = output1_norm.transpose(0, 1)\n",
    "        # output2_transposed = output2_norm.transpose(0, 1)\n",
    "\n",
    "        # # get batch similarity matrix\n",
    "        # matrix = torch.bmm(output1_transposed, output2_transposed.transpose(1, 2))\n",
    "        # # exp for softmax\n",
    "        # matrix_exp = torch.exp(matrix / TEMPERATURE)\n",
    "\n",
    "        # matrix_sum = torch.sum(matrix_exp, dim=2)\n",
    "\n",
    "        # # get the diagonal mask to extract the similarity of same sentences\n",
    "        # mask = torch.eye(matrix_exp.shape[1]).to(device).unsqueeze(0).repeat(matrix_exp.shape[0], 1, 1)\n",
    "\n",
    "        # correct_similarity = torch.sum(matrix_exp * mask, dim=2)\n",
    "\n",
    "        # # calcluate the ratio of same sentence similarity to all similarity\n",
    "        # ratio = correct_similarity / matrix_sum\n",
    "\n",
    "        # # calculate the cross entropy loss\n",
    "        # # then multiply by the eps array to give recent tokens more weight\n",
    "        # bits = -torch.log(ratio).transpose(0, 1) * eps_array[:, :length]\n",
    "\n",
    "        # # average the loss\n",
    "        # loss = torch.mean(bits)\n",
    "\n",
    "        # if loss is nan, break\n",
    "        # if torch.isnan(loss):\n",
    "        #     print('nan')\n",
    "        #     break\n",
    "\n",
    "        # print(loss.item())\n",
    "        # break\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\n",
    "            'loss': loss.item(),\n",
    "        })\n",
    "        if i % 500 == 0:\n",
    "            validate(model, i + 1)\n",
    "        if i % 1000 == 0:\n",
    "            log_graph(model, i + 1)\n",
    "        if i % 2000 == 1999:\n",
    "            model.save_pretrained(f'/workspace/results/weighted_mask_models/{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/workspace/results/weighted_mask_models/last')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
